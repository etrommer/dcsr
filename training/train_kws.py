import os
import argparse
import logging

import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
import mlflow

import dense_model
import prune
from preprocess import get_spectrogram
from util import plot_history

def log_history(history):
    mlflow.log_metric('Training Loss', history.history['loss'][-1])
    mlflow.log_metric('Training Accuracy', history.history['accuracy'][-1])
    mlflow.log_metric('Validation Loss', history.history['val_loss'][-1])
    mlflow.log_metric('Validation Accuracy', history.history['val_accuracy'][-1])

    plot_history(history)
    mlflow.log_artifact('training_history.pdf')

def main(architecture, training_epochs, pruning_epochs, retraining_epochs, dense_weights_path):
    AUTOTUNE = tf.data.experimental.AUTOTUNE
    mlflow.set_experiment(experiment_name='Wakeword DS-CNN Pruning')

    train_ds = tfds.load('speech_commands', data_dir='.', split='train', as_supervised=True)
    val_ds, ds_info = tfds.load('speech_commands', data_dir='.', split='validation', with_info=True, as_supervised=True)
# test_ds = tfds.load('speech_commands', data_dir='.', split='test', as_supervised=True)

    commands = tf.constant(ds_info.features['label'].names)
    num_labels = len(commands)

    train_ds = train_ds.map(lambda x,y: (get_spectrogram(x), y), num_parallel_calls=AUTOTUNE)
    val_ds = val_ds.map(lambda x,y: (get_spectrogram(x), y), num_parallel_calls=AUTOTUNE)

    for sample, _ in train_ds.take(1):
        input_shape = sample.shape
        logging.debug('Input Shape: {}'.format(input_shape))
# test_ds = test_ds.map(lambda x,y: (get_spectrogram(x), y), num_parallel_calls=AUTOTUNE)

    batch_size = 128
    train_ds = train_ds.batch(batch_size).cache().prefetch(AUTOTUNE)
    val_ds = val_ds.batch(batch_size).cache().prefetch(AUTOTUNE)

    # Try loading weights from disk and retrain if they are not present
    model = dense_model.ds_cnn_model(architecture, input_shape, num_labels)
    try:
        model.load_weights(dense_weights_path)
        logging.debug('Using weights in {} for base model'.format(dense_weights_path))
    except Exception as e:
        logging.debug('Loading weights from file {} failed with {}'.format(dense_weights_path, e))
        logging.debug('Training Dense Base Model from scratch')
        dense_weights_path = 'dense_model_{}.hdf5'.format(architecture)

        with mlflow.start_run(run_name='Train Base Model'):
            mlflow.log_param('Architecture', architecture)
            mlflow.log_param('Epochs', training_epochs)
            mlflow.log_param('Batch Size', batch_size)

            history = dense_model.train(model, train_ds, val_ds, dense_weights_path, epochs=training_epochs)
            log_history(history)
            mlflow.keras.log_model(model, 'models')

    # Run sweep over different sparsities
    target_sparsities = [0.7, 0.75, 0.8, 0.85, 0.9, 0.95]
    for target_sparsity in target_sparsities:
        logging.debug('Pruning to {}% sparsity'.format(int(target_sparsity * 100)))
        with mlflow.start_run(run_name='Pruning'):
            mlflow.log_param('Architecture', architecture)
            mlflow.log_param('Epochs', pruning_epochs)
            mlflow.log_param('Batch Size', batch_size)
            mlflow.log_param('Sparsity', target_sparsity)
            mlflow.log_param('Retraining Epochs', retraining_epochs)

            sparse_model, history = prune.prune(
                model,
                train_ds,
                val_ds,
                pruning_epochs=pruning_epochs,
                retraining_epochs=retraining_epochs,
                target_sparsity=float(target_sparsity)
            )
            log_history(history)
            mlflow.keras.log_model(sparse_model, 'models')

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Training and Pruning for Keyword Spotting DS-CNN'
    )
    parser.add_argument(
        '-e', '--epochs', type=int, default=60,
        help='Number of Training Epochs for Dense Base Model'
    )
    parser.add_argument(
        '-p', '--pruning_epochs', type=int, default=40,
        help='Number of Pruning Epochs'
    )
    parser.add_argument(
        '-r', '--retraining_epochs', type=int, default=20,
        help='Number of Retraining Epochs after Pruning'
    )
    parser.add_argument(
        '-a', '--architecture', type=str.lower, default='s', choices=['s', 'm', 'l'],
        help='Model Architecture as described in \'Hello Edge\' paper'
    )
    parser.add_argument(
        '-w', '--weights', type=str, default=None,
        help='Path to weights for Dense Base Network. Weights need to be generated by the same architecture as given in the \'architecture\' parameter. Providing weights skips the initial training of a dense base model.'
    )
    parser.add_argument(
        '-d', '--debug', action='store_const', dest='loglevel', const=logging.DEBUG, default=logging.WARNING,
        help='Print Debug Output',
    )
    args = parser.parse_args()
    logging.basicConfig(level=args.loglevel)
    main(args.architecture, args.epochs, args.pruning_epochs, args.retraining_epochs, args.weights)
